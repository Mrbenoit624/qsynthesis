#!/usr/bin/env python3

from itertools import permutations, combinations
import random
from pathlib import Path
import logging
import click
import time
from binascii import unhexlify

import qsynthesis
from qsynthesis.grammar import TritonGrammar, BvOp
from qsynthesis.tables import HashType, LookupTableRaw, LookupTableLevelDB


def biased_input_generator(bitsize: int, var_num: int, input_number: int, bs: int, random_level: int = 2):
    n = max(var_num-3, random_level)
    minus_one = pow(2, bitsize)-1
    vals = [1, 0, minus_one] + [None] * n
    all_perms = list(permutations(vals, var_num))
    return list(map(lambda l: [random.getrandbits(bs) if x is None else x for x in l], random.sample(all_perms, k=input_number)))


def operator_generator(nb_operator: int):
    """ Generate infinite operators set (making sure all combinations  """
    ops = [BvOp.NOT, BvOp.AND, BvOp.OR, BvOp.XOR, BvOp.NEG, BvOp.ADD, BvOp.MUL, BvOp.SUB]
    while 1:
        l = list(combinations(ops, nb_operator))
        random.shuffle(l)
        yield from l


@click.group(context_settings={'help_option_names': ['-h', '--help']})
@click.version_option(version=qsynthesis.__version__, message='%(version)s')
def main():
    pass


@main.command(name="generate")
@click.argument('output_file', type=str)
@click.option('--resume', metavar="resume file", type=str, help='resume resume lookup table generation from given file')
@click.option('-bs', '--bitsize', metavar="bitsize", default=64, type=int, help="Bit size of expressions")
@click.option('--var-num', default=3, type=int, help="Number of variables")
@click.option('--input-num', default=5, type=int, help="Number of inputs")
@click.option('--random-level', type=int, default=2, help="Randomness level of inputs 0 means higlhly biased to use corner-case values (0,1,-1)")
@click.option('--max-depth', default=5, type=int, help="Maximum depth")
@click.option('--op-num', default=5, type=int, help="Operator number")
@click.option("-v", "--verbosity", default=0, count=True, help="increase output verbosity")
@click.option('-k', default=1, type=int, help="Number of tables to generate")
@click.option('--ops', type=str, default='', help='specifying operators to uses')
@click.option('--inputs', type=str, default='', help='specifying input vector to use')
@click.option('--hash-mode', default=HashType.RAW, type=click.Choice([x.name for x in HashType]), help="Hash function for keys in table")
@click.option('--watchdog', type=float, help="Activate RAM watchdog (percentage of load when to stop)")
@click.option('-t', '--type', type=click.Choice(["db", "bin"]), default="db", help="Type of database to create")
@click.option('--linearization', is_flag=True, type=bool, default=False, help="If set activate linearization of expressions")
def generate_command(output_file, resume, bitsize, var_num, input_num, random_level, max_depth, op_num, verbosity, k, ops, inputs, hash_mode, watchdog, type, linearization):
    """ Table generation utility """
    logging.basicConfig(level=logging.DEBUG if verbosity else logging.INFO, format='%(message)s')

    try:
        import pydffi
    except ImportError:
        raise click.Abort("Cannot import dragonffi (pip3 install pydffi")
    try:
        import sympy
    except ImportError:
        raise click.Abort("Cannot import sympy (pip3 install sympy")

    if resume is not None:
        pass  # TODO: implementing resume features
    else:
        out_dir = Path(output_file)
        if out_dir.exists() and out_dir.is_file():
            out_dir.unlink()
        if not out_dir.exists() and k > 1:
            out_dir.mkdir()

        ops = [BvOp[x] for x in ops.split(",")] if ops else None
        inputs = [int(x) for x in inputs.split(",") if x]

        t1 = time.time()

        for i in range(k):
            logging.info(f"Generate Table #{i}")
            p = out_dir/f"{i}.{type}" if k > 1 else out_dir

            operators = next(operator_generator(op_num)) if ops is None else ops
            vrs = [chr(ord('a') + x) for x in range(var_num)]
            if inputs:
                inputs = [{n: v for n, v in zip(vrs, inputs[i:i + len(vrs)])} for i in
                          range(0, len(inputs), len(vrs))]
            else:
                inputs = biased_input_generator(bitsize, var_num, input_num, bitsize, random_level)
                inputs = [{n: v for n, v in zip(vrs, i)} for i in inputs]

            grammar = TritonGrammar([(x, bitsize) for x in vrs], operators)

            logging.info(f"Watchdog value: {watchdog}")
            mp = {"db": LookupTableLevelDB, "bin": LookupTableRaw}
            ltm = mp[type].create(p.absolute(), grammar, inputs, HashType[hash_mode])
            try:
                if watchdog:
                    ltm.generate(depth=max_depth, do_watch=True, watchdog_threshold=watchdog, linearize=linearization)
                else:
                    ltm.generate(depth=max_depth, linearize=linearization)
            except KeyboardInterrupt:
                logging.warning("Stop required")

            ltm.save(p)
        elapsed = time.time() - t1
        hours, rem = divmod(elapsed, 3600)
        minutes, seconds = divmod(rem, 60)
        logging.info(f"\n{int(hours)}h{int(minutes)}m{seconds:.2f}s")


@main.command(name="info")
@click.argument('table_file', type=click.Path(exists=True))
def infos_command(table_file):
    """Getting information of a given database"""
    logging.basicConfig(level=logging.INFO, format='%(message)s')
    table_file = Path(table_file)

    if table_file.suffix == ".bin":
        table = LookupTableRaw.load(table_file)
    else:
        table = LookupTableLevelDB.load(table_file)

    logging.info(f"Bitsize: {table.bitsize}")
    logging.info(f"Hash mode: {table.hash_mode.name}")
    logging.info(f"Size: {table.size}")
    logging.info(f"Variables: {table.grammar.vars}")
    logging.info(f"Operators: {[x.name for x in table.grammar.ops]}")
    logging.info(f"Nb inputs: {len(table.inputs)}")
    l = []
    for i in table.inputs:
        for v in i.values():
            l.append(v)
    logging.info(",".join(str(x) for x in l))


@main.command(name="check")
@click.argument('table_file', type=click.Path(exists=True))
def check_command(table_file):
    """Checking the equivalence of hashes against evaluation of expressions on inputs"""
    logging.basicConfig(level=logging.INFO, format='%(message)s')
    table_file = Path(table_file)

    table = LookupTableLevelDB.load(table_file)
    count = table.size
    good, bad = 0, 0

    for i, (h, expr) in enumerate(table):
        if i % 100 == 0:
            print(f"process {i}/{count} [KO:{bad}]\r", end="")
        triton_exp = table.get_expr(expr)
        outs = table.eval_expr_inputs(triton_exp)
        if table.hash(outs) != h:
            logging.warning(f"Bad expression: {expr}  with [{outs}]")
        else:
            good += 1
    logging.info(f"[OK:{good}/{count}]{'': <15}")


@main.command(name="runserver")
@click.argument('table_file', type=click.Path(exists=True))
@click.option('-p', '--port', type=int, default=8080, help="Service port to listen on")
def serve_command(table_file, port):
    """Run the REST API to serve a given table database"""
    logging.basicConfig(level=logging.INFO, format='%(message)s')
    table_file = Path(table_file)

    try:
        table = LookupTableLevelDB.load(table_file)
    except IOError:
        click.Abort("Lookup table database is invalid")

    logging.info("preload table size")

    try:
        from fastapi import FastAPI, Query
        import uvicorn
    except ImportError:
        raise click.Abort("Cannot import fastapi or uvicorn (pip3 install fastapi uvicorn")

    app = FastAPI()

    @app.get("/")
    def read_root():
        return {'size': table.size,
                'hash_mode': table.hash_mode.name,
                'inputs': table.inputs,
                'grammar': table.grammar.to_dict()}

    @app.get("/entry/{hash}")
    def read_item(hash: str = Query(None, min_length=32, max_length=32, regex="^[0-9a-z]+$")):
        decoded = unhexlify(hash)
        entry = table.lookup_hash(decoded)
        print(entry)
        if entry:
            return {"hash": hash, "expression": entry}
        else:
            return {}

    uvicorn.run(app, host="0.0.0.0", port=port)


@main.command(name="compare")
@click.argument('table1', type=click.Path(exists=True))
@click.argument('table2', type=click.Path(exists=True))
def compare_command(table1, table2):
    """Compare two tables"""
    table1 = LookupTableLevelDB.load(table1)
    table2 = LookupTableLevelDB.load(table2)

    only1 = 0
    only2 = 0
    common = 0
    sz1 = table1.size
    sz2 = table2.size
    for h, k in table1:
        if table2.db.get(h):
            common += 1
        else:
            only1 += 1
    for h, k in table2:
        if not table1.db.get(h):
            only2 += 1

    print(f"Table 1 size:{sz1}\tTable 2 size:{sz2}\t[Inputs:{'OK' if table1.inputs == table2.inputs else 'DIFFERENT'}]")
    print(f"Only table 1:{only1}\tOnly table2:{only2}\tCommons:{common}")
    # FUTURE: Implementing semantic comparison of common keys


@main.command(name="import")
@click.argument('raw_table', type=click.Path(exists=True))
@click.argument('out_table', type=click.Path(exists=False))
def import_command(infile, outfile):
    """Import a raw table in a Level-DB database"""
    f_id = 1
    current_file = infile
    out_lkp = None
    i = 0
    buffer = []

    while True:
        lkp = LookupTableRaw.load(current_file)
        if out_lkp is None:
            out_lkp = LookupTableLevelDB.create(outfile, lkp.grammar, lkp.inputs, lkp.hash_mode)
        try:
            for it in lkp:
                buffer.append(it)
                i += 1
                if i % 100000 == 0:
                    print(f"process: {i}\r", end="")
                if i % 5000000 == 0:
                    out_lkp.add_entries(buffer, chunk_size=5000000, update_count=False)
                    buffer = []
        except Exception as e:
            print(f"Exception raised: {e} stop.")
            break
        except KeyboardInterrupt as _:
            print("Keyboard interrupt")
            break

        p = Path(f"{infile}.{f_id}")
        if p.exists():
            print(f"\n Start processing: {p}")
            current_file = str(p)
            f_id += 1
        else:
            break
    # flush remaining entries
    out_lkp.add_entries(buffer,  chunk_size=30000, update_count=False)
    out_lkp.db.put(b"size", str(i).encode())
    print(f"Imported: {i}")


@main.command(name="merge")
@click.argument('in_table', type=click.Path(exists=True))
@click.argument('out_table', type=click.Path(exists=False))
def merge_command(in_table, out_table):
    """Merge entries of the first database in the second"""
    lkp_in = LookupTableLevelDB.load(in_table)
    lkp_out = LookupTableLevelDB.load(out_table)

    i = 0
    c = 0
    sz = lkp_in.size
    for hash, s in lkp_in:
        if lkp_out.db.get(hash) is None:
            lkp_out.add_entry(hash, s)
            i += 1
        c += 1
        if c % 100 == 0:
            print(f"count:{c}/{sz} (imported:{i})\r", end="")

    print(f"Imported: {i}")


if __name__ == "__main__":
    main()
